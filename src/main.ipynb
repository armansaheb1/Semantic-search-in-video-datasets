{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e0748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import weaviate\n",
    "from extract_data_all_files import extract_metadata_and_sentences\n",
    "from frame_desc_all import process_video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5fcd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_directory, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Get the list of VRT files in the input directory\n",
    "    vrt_files = [f for f in os.listdir(input_directory) if f.endswith('.vrt')]\n",
    "\n",
    "    # Process each VRT file\n",
    "    for file_name in vrt_files:\n",
    "        # Create the input file path\n",
    "        input_file = os.path.join(input_directory, file_name)\n",
    "\n",
    "        # Extract metadata and sentences\n",
    "        video_data = extract_metadata_and_sentences(input_file)\n",
    "\n",
    "        # Create the output file path\n",
    "        output_file = os.path.join(output_folder, f\"{file_name[:-4]}.json\")\n",
    "\n",
    "        # Save the result as JSON\n",
    "        with open(output_file, 'w') as file:\n",
    "            json.dump(video_data, file, indent=1)\n",
    "\n",
    "        print(f\"Extraction completed. Output file: {output_file}\")\n",
    "    \n",
    "    video_files = [f for f in os.listdir(input_directory) if f.endswith('.mp4')]\n",
    "    \n",
    "    for file_name in video_files:\n",
    "        input_file = os.path.join(input_directory, file_name)\n",
    "        json_file = os.path.join(output_folder, f\"{file_name[:-4]}.v4.json\")\n",
    "        process_video_frames(input_file, json_file, \"../frames/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5497fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'input_files` should only contain new .vrt and  their corresponding .mp4 files for which you want to populate data, delete already existing files to avoid duplicate insertions, as the below function will run all files which are there in input_files folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99e0c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_file ../input_files/Second Persian Invasion.mp4\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf6 in position 26: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../input_files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../output_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# Here input_files is input directory containing videos and vrt files.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(input_directory, output_folder)\u001b[0m\n\u001b[1;32m     12\u001b[0m input_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_directory, file_name)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Extract metadata and sentences\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m video_data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_metadata_and_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create the output file path\u001b[39;00m\n\u001b[1;32m     18\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Semantic-search-in-video-datasets/src/extract_data_all_files.py:10\u001b[0m, in \u001b[0;36mextract_metadata_and_sentences\u001b[0;34m(input_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Read the input text from the file\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 10\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     12\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf6 in position 26: invalid start byte"
     ]
    }
   ],
   "source": [
    "main(\"../input_files\", \"../output_data\")   # Here input_files is input directory containing videos and vrt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "client.schema.get()  # For the first time you will get classes: [] as you have no classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32815b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def get_embedding(text):\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce85a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_obj = {\n",
    "    \"class\": \"Video_text\"\n",
    "}\n",
    "client.schema.create_class(class_obj)  # This will give error if class already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_obj1 = {\n",
    "    \"class\": \"Video_text_description\"\n",
    "}\n",
    "client.schema.create_class(class_obj1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20259b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_obj2 = {\n",
    "    \"class\": \"Video_description\"\n",
    "}\n",
    "client.schema.create_class(class_obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ee232",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'output_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [f for f in os.listdir(input_directory) if f.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list will show json files that you will be populating. Verify once that this files are already not populated to avoid duplicate data.\n",
    "json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that json_files you have in variable 'json_files' have already not been populated. Otherwise it will insert duplicate records in weaviate.\n",
    "\n",
    "for file_name in json_files:\n",
    "    input_file = os.path.join(input_directory, file_name)\n",
    "    with open(input_file) as f:\n",
    "        data = json.load(f)  \n",
    "        metadata = data['metadata']['file']\n",
    "        video_id = data['metadata']['text_id'][3:]\n",
    "        with client.batch(batch_size=100) as batch:\n",
    "            \n",
    "            for sent in data['sentences']:\n",
    "                \n",
    "                embedding_video_text = model.encode(sent['sentence'])\n",
    "                properties_text = {\n",
    "                   \"text\": sent['sentence'],\n",
    "                   \"starttime\" : sent['starttime'],\n",
    "                   \"endtime\" : sent['endtime'],\n",
    "                   \"metadata\" : metadata,\n",
    "                   \"video_id\" : video_id\n",
    "                }\n",
    "                \n",
    "                client.batch.add_data_object(\n",
    "                    properties_text,\n",
    "                    \"Video_text\",\n",
    "                    vector = embedding_video_text\n",
    "                )\n",
    "                                \n",
    "                combined_text = \"In the video you can hear: \" + sent['sentence'] + \" In the video you can see: \" + \", \".join([sentence.strip(\" .\") for sentence in sent['frame_data']]) + '.'\n",
    "                embedding_video_text_desc = model.encode(combined_text)\n",
    "                properties_video_text_desc = {\n",
    "                   \"text\": combined_text,\n",
    "                   \"starttime\" : sent['starttime'],\n",
    "                   \"endtime\" : sent['endtime'],\n",
    "                   \"metadata\" : metadata,\n",
    "                   \"video_id\" : video_id\n",
    "                }\n",
    "                client.batch.add_data_object(\n",
    "                    properties_video_text_desc,\n",
    "                    \"Video_text_description\",\n",
    "                     vector = embedding_video_text_desc\n",
    "                )\n",
    "                \n",
    "                video_desc = \", \".join([sentence.strip(\" .\") for sentence in sent['frame_data']]) + '.'\n",
    "                embedding_video_desc = model.encode(video_desc)\n",
    "                properties_video_desc = {\n",
    "                   \"text\": video_desc,\n",
    "                   \"starttime\" : sent['starttime'],\n",
    "                   \"endtime\" : sent['endtime'],\n",
    "                   \"metadata\" : metadata,\n",
    "                   \"video_id\" : video_id\n",
    "                }\n",
    "                client.batch.add_data_object(\n",
    "                    properties_video_desc,\n",
    "                    \"Video_description\",\n",
    "                     vector = embedding_video_desc\n",
    "                )\n",
    "    print(\"file done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabfcfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.query.aggregate(\"Video_text_description\").with_meta_count().do()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
